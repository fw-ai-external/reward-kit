# Next set of ticket items

## âœ… Support DeepSeek-V2 Prover reward function
- COMPLETED: Implemented three reward functions for Lean theorem proving:
  1. `lean_prover_reward`: Basic evaluation of Lean proofs
  2. `deepseek_prover_v2_reward`: Enhanced evaluation focusing on subgoal decomposition
  3. `deepseek_huggingface_prover_benchmark`: Evaluation against HuggingFace datasets
- Documentation and examples added in `/docs/examples/deepseek_prover_v2.md`
- Example code available in `/examples/deepseek_prover_v2_example.py`

## Running with HuggingFace Datasets

Currently only deepseek prover supports huggingface datasets. To evaluate proofs against the DeepSeek-ProverBench dataset:

```python
from reward_kit.rewards.lean_prover import deepseek_huggingface_prover_benchmark

# Install HuggingFace datasets dependency first:
# pip install "reward-kit[deepseek]"

# Evaluate a proof against the DeepSeek-ProverBench dataset
result = deepseek_huggingface_prover_benchmark(
    response="your_lean_proof_here",
    statement="For all natural numbers n, the sum of the first n integers equals n(n+1)/2",
    dataset_name="deepseek-ai/DeepSeek-ProverBench"
)

print(f"Score: {result.score}")
```

The function will automatically search for a matching problem in the dataset based on the statement.

But this is ugly, and for running evaluation preview and evaluation job, we do not automatically handle conversion from huggingface dataset to jsonl right now, ideally we update the API to do that so then the reward functions 
can continue to stay oblivious to huggingface dataset concepts.

## TRL adapter for reward functions

We want to make sure the reward functions can be used with TRL as well with GRPO. I downloaded the grpo trainer code into TRL cookbooks, please check it out before implementing the TRL adapter and then make sure our reward functions can be used inside TRL as well.