# Reward-Kit SDK Issues

# High pri issues

## Developer have an hard time understanding error types

```
bennychen@Bennys-MacBook-Air reward-kit % PYTHONPATH=. python3 examples/math_example/fireworks_regenerate.py --regenerate-recorded-data
....

--- Regeneration & Evaluation Summary ---
Total samples attempted in this run: 100
Samples passed (among attempted): 86
Samples failed (among attempted): 14

Some samples failed or an error occurred during regeneration/evaluation for this run.

Successfully saved recorded API calls to /Users/bennychen/Documents/reward-kit/examples/math_example/fireworks_regenerate_recorded_data.jsonl
```

We need to help people classify the error types.

## TRL ref model
I think I need to pick a reasonable ref mode for GRPO? Let's make sure we dig to come up with a reasonable default here.

## Data filtering script still not filtering things properly

```
python scripts/convert_hf_math_to_openai_jsonl.py open-r1/OpenR1-Math-220k ./runs/openr1_numeric.jsonl --filter_by_match --math_type numeric
```

for example this row passed

```
{
  "messages": [
    {
      "role": "user",
      "content": "(7) Let $z \\in \\mathbf{C}$, and satisfy $|z-\\mathrm{i}| \\leqslant 1, A=\\operatorname{Re}(z)\\left(|z-\\mathrm{i}|^{2}-1\\right)$. Find $\\max A$ and $\\min A$."
    },
    {
      "role": "assistant",
      "content": "(Answer: When $z=-\\frac{\\sqrt{3}}{3}+\\mathrm{i}$, $\\max A=\\frac{2 \\sqrt{3}}{9}$; when $z=\\frac{\\sqrt{3}}{3}+\\mathrm{i}$, $\\min A=-\\frac{2 \\sqrt{3}}{9}$ )"
    }
  ],
  "ground_truth_answer_from_column": "\\maxA=\\frac{2\\sqrt{3}}{9},\\A=-\\frac{2\\sqrt{3}}{9}",
  "match_details": {
    "filter_passed": true,
    "reward_score": 1,
    "match_comparison_reason": "Best match: Gen='2' (2.0) vs Orig='2' (2.0).\nNumeric match: Yes, Similarity: 1.000",
    "math_type_used_for_filter": "numeric",
    "extracted_from_solution_column": "Extracted from generated: '3' (3.0), '3' (3.0), '2' (2.0), '3' (3.0), '9' (9.0), '3' (3.0), '3' (3.0), '2' (2.0), '3' (3.0), '9' (9.0)",
    "extracted_from_gt_answer_column": "Extracted from original: '2' (2.0), '3' (3.0), '9' (9.0), '2' (2.0), '3' (3.0), '9' (9.0)"
  }
}
```

you can see the ground truth answer is not even a numerical number, but it is getting filtered

# Mid pri issues

## Move deepseek deps into coding
Should be part of main package

## `preview_evaluation` API error: "unknown field assertionType" (2025-05-15)
When running `examples/evaluation_preview_example.py`, the call to `preview_evaluation` (in `reward_kit.evaluation`) fails with a 400 Bad Request from the API endpoint `https://api.fireworks.ai/v1/accounts/fireworks/evaluators:previewEvaluator`.
The error response is:
```json
{
  "code": 3,
  "details": [],
  "message": "proto: (line 1:125): unknown field \"assertionType\""
}
```
The script currently falls back to a simulated preview mode. This might indicate an issue with the request payload generated by `reward_kit.evaluation.preview_evaluation` or a server-side API incompatibility.

## `examples/agent_evaluation_example.py` is outdated (2025-05-15)
The example script `examples/agent_evaluation_example.py` attempts to import `load_task_from_file` and `AgentEvaluator` from `reward_kit.agent`. These components are not found in the current `reward_kit.agent` module, which exposes V2 framework components like `Orchestrator` and `ForkableResource`.
The example script is therefore broken and needs to be updated to use the V2 agent evaluation framework or be removed if obsolete.

# Low pri issues

## âœ… Delete all legacy_reward_function
Removed legacy_reward_function from reward_function.py and __init__.py since it's no longer needed.

## Fix the E2B unittest
```
tests/test_deepcoder_reward.py ....s.s...                                                                  [ 55%]
tests/test_fractional_code.py .....s..                                                                     [100%]
```
some of the tests are skipped due to E2B not working for me earlier
