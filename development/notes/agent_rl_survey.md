Got it. I’ll look into VeRL-Agent, ARTIST from Microsoft, the original VeRL project, and SkyRL to identify best practices and ideas for implementing multi-step rollout setups. I’ll focus on strategies that could integrate well with your framework’s strength in making reward functions easy to define.

I’ll update you shortly with a detailed summary and recommendations.


# Introduction

Multi-step rollouts are a cornerstone of advanced reinforcement learning (RL) systems, especially for tasks where an agent must take a sequence of actions before receiving feedback. Traditional RLHF setups for language models often consider single-turn interactions (one prompt → one response with a reward), but emerging frameworks now tackle **multi-turn or multi-step episodes**. In a multi-step rollout, an agent interacts with an environment over several time steps (or dialog turns) before the episode ends, which introduces challenges in **temporal abstraction**, **credit assignment for delayed rewards**, and **training loop design**. We examine how four cutting-edge RL frameworks handle multi-step rollouts: **VeRL (Volcano Engine RL)** and its open-source agent implementation, **VeRL-Agent (GiGPO)**; Microsoft’s **ARTIST** framework; and **SkyRL** from Berkeley. We explore each project’s design for multi-step interactions – focusing on their temporal abstraction strategies, rollout scheduling, delayed reward handling, reward propagation, and training loop modifications – and then highlight best practices that could enhance an RL framework already strong in reward function authoring. Relevant documentation, papers, and code repositories are cited for deeper reference.

## Multi-Step Rollouts in VeRL (Volcano Engine RL)

**VeRL** (Volcano Engine RL) is a production-grade RL training framework originally designed for fine-tuning large language models (LLMs) with RLHF. In its initial design, VeRL focused on single-turn trajectories (e.g. prompt → model generates an answer → receive a scalar reward) typical of RLHF. However, recent updates have extended VeRL to support **multi-turn dialogues and multi-step tasks**. This extension was achieved by integrating an **asynchronous rollout engine** (developed by the SGLang team) that allows the model to engage in multi-turn interactions with tools or environments. In practice, enabling multi-step episodes in VeRL is as simple as toggling a config flag – e.g. `multi_turn: True` – which switches the rollout backend to the SGLang asynchronous engine. With this setup, a single “episode” can consist of *multiple* query-response turns or tool API calls, rather than a single generation.

**Temporal Abstraction:** VeRL’s approach to temporal abstraction in multi-step mode is relatively straightforward – it treats each step (e.g. one conversational turn or one tool invocation) as a time-step in the episode. There isn’t a built-in hierarchy of macro-actions (options) yet; the policy still decides an action step-by-step. Temporal abstraction is achieved implicitly by letting the *LLM’s policy output sequences of tokens* for each action. For instance, the model might output a chunk of text constituting a full query or command in one step, which is then executed in the environment. This means one high-level action can encompass many token-level generations (e.g. a multi-word question or a code snippet) – effectively a form of abstraction at the token level. The VeRL pipeline ensures the model can **“think” and act in multiple turns** within one episode, but it doesn’t predefine higher-level skills; any abstraction arises from how the model and reward function are set up by the user. (Notably, ongoing work in the VeRL ecosystem hints at more advanced temporal abstractions like “partial rollouts” and multi-turn *agentic* behavior, but those are experimental.)

**Rollout Scheduling and Execution:** VeRL uses a distributed architecture (built on Ray) to handle rollouts. In multi-turn mode, rollout *workers* maintain an environment state and loop through model→environment interactions until an end-of-episode condition is met. The introduction of the SGLang backend allows these rollouts to run **asynchronously and in parallel**, which is vital for long, multi-step episodes. Asynchronous rollout means the framework can overlap computation and action execution: while one agent’s turn is being processed (e.g. the model thinking or generating text), another environment instance might be advancing. VeRL’s design supports multi-node and multi-GPU execution, so many episodes can be generated concurrently. This **parallel rollout scheduling** prevents long-horizon episodes from becoming a training bottleneck. In fact, VeRL’s integration with SGLang provides the same optimizations as its single-turn mode (like fast token generation using vLLM or other inference engines) now applied to iterative interactions. The result is a scalable rollout system where even multi-turn dialogues or tool-use sequences can be generated at high throughput. Recent releases demonstrated this capability, announcing a “fully functional… end-to-end open source multi-turn RLHF framework” with **asynchronous multi-turn dialogues and tool-calling** support integrated into VeRL.

**Delayed Rewards and Reward Structure:** In VeRL, the reward for an episode is typically computed at the end of the trajectory. Originally, this might be a scalar from a reward model comparing the final answer to a reference (as in RLHF). With multi-step rollouts, the task might have **delayed rewards** – e.g. success/failure only determined after several turns. VeRL’s philosophy is to make reward function authoring flexible. Users can plug in a custom **RewardManager** that assigns rewards at the end or even at intermediate steps if desired. By default, if only a final outcome is available (a sparse reward), VeRL will use the total return at episode end for learning. Under the hood, it relies on **PPO with GAE (Generalized Advantage Estimation)** to propagate that reward back through the sequence of model decisions. In other words, even if a reward is given only at episode completion, the algorithm computes advantages for each action in the trajectory by estimating the expected future reward (which in a terminal state is just the final reward) and applying discounting/λ-returns. This way, every action in the multi-step trajectory gets some credit assigned proportional to its contribution to the final outcome. If intermediate rewards are specified (e.g. a small reward for each sub-goal achieved), VeRL will simply sum discounted rewards over the trajectory as usual. The key point is that **VeRL doesn’t hard-code how rewards are obtained** – whether the reward comes only at the last step or at multiple steps, the PPO trainer will handle it through the return computation mechanism. For instance, on a multi-turn question-answering dataset like GSM8K (math problems), one could give a reward +1 only if the final answer is correct, which VeRL would treat as a delayed reward scenario.

**Reward Propagation:** With primarily outcome-based rewards, VeRL leans on **algorithmic credit assignment** to handle propagation. PPO with advantage estimation means that even in a long sequence, the final positive reward will **increase the probability of all the actions that led to it**, albeit scaled by discount factors. If a sequence is very long or the reward extremely sparse, credit assignment can become noisy – which is why advanced techniques (see VeRL-Agent below) have been explored. Out of the box, VeRL’s propagation is equivalent to standard n-step returns: by adjusting the discount factor, one can control how far back rewards are propagated. For example, a high discount (close to 1.0) ensures even very early actions still get some credit from a final reward. Additionally, because VeRL is built for RLHF, it often uses a reference baseline (like a value function or a reward model baseline) to reduce variance in the reward signal, similar to how PPO in RLHF setups uses a value network to compute advantages. In summary, **VeRL uses standard PPO credit assignment** for multi-step rollouts – effective for many cases, but it relies on the reward function design to provide any shaping if needed. The framework’s strength here is that since reward authoring is easy to customize, users can inject domain knowledge: e.g., provide intermediate rewards for subtask completion to help credit assignment, if the domain permits.

**Training Loop Modifications:** Supporting multi-step rollouts required some adjustments to VeRL’s training loop. In single-turn RLHF, a rollout might simply be: sample a prompt, have the model generate a completion, then compute reward. In multi-turn mode, the **rollout worker** now must encapsulate an *interaction loop*: for each episode, repeatedly: send observation to the model → get action (text/tool call) → apply action to env → get new observation/reward (if any) → continue until done. VeRL implemented this by introducing a new rollout worker class (`sglang_async`) that can maintain dialogue state and handle tool APIs. The training loop on the central learner remains similar – it gathers complete trajectories and then performs PPO updates. But now those trajectories are longer and contain sequences of (state, action, reward) instead of just one action. VeRL’s dataflow design (HybridFlow) already treats RL as a data pipeline, so extending the pipeline to multiple steps was natural. Each trajectory, once collected, is fed into the PPO updater which computes losses using the sequence of log probabilities, value estimates, and rewards. One subtle point is that when the “environment” is a tool API or a conversation, the state and action need to be represented as text. VeRL handles this by defining a **prompt template or schema** for the multi-turn interaction so that the entire trajectory can be serialized (e.g., a transcript with special tokens for state and action). This is analogous to what ARTIST does (discussed next), though VeRL leaves the specifics of the prompt format to the user’s environment setup. In summary, VeRL’s training loop was modified to incorporate a **while-loop for rollouts** and an async dispatch of those rollouts, but the learning update (PPO on aggregated trajectories) stays the same. The successful integration of multi-turn support in VeRL is evidenced by the recent announcement of an end-to-end multi-turn RLHF solution that supports **multi-task and multi-tool training** on the same platform.

## VeRL-Agent (GiGPO): Long-Horizon Credit Assignment

While VeRL provides the infrastructure for multi-step RL, **VeRL-Agent** refers to an example agent (and algorithm) implementation built on VeRL that addresses some challenges of long-horizon training. One notable approach is described in the paper *“Group-in-Group Policy Optimization for LLM Agent Training”*, which introduced **GiGPO**. GiGPO is implemented in an open-source repository `verl-agent`. Its primary contribution is a strategy to improve credit assignment for very long multi-step trajectories, especially when rewards are sparse or delayed until the end.

**Temporal Abstraction Strategy:** GiGPO does not introduce new action primitives or hierarchical policies; instead, its temporal abstraction is in the *credit assignment and optimization process*. It introduces a two-level grouping of experiences: **episode-level** and **step-level**. At the episode level, it treats the entire trajectory (from start to terminal outcome) as one unit to evaluate overall success (a “macro” perspective). At the step level, it looks at individual state-action decisions (a “micro” perspective). This can be seen as temporal abstraction in credit assignment: the algorithm distinguishes between the **whole trajectory outcome** and the **per-step action quality**. By doing so, it can address long-horizon credit assignment in a more structured way than standard PPO. The agent’s policy is still operating at the base level (choosing an action each step), but the learning signal is abstracted at two time-scales: the full horizon and the one-step transition.

**Rollout Scheduling:** To enable its group-based credit computation, GiGPO’s **rollout scheduling** involves sampling *groups of trajectories under identical conditions*. In practice, the algorithm begins an iteration by resetting multiple environment instances to the **same initial state** and having the agent roll out trajectories in each. For example, imagine 10 parallel environments all starting from the same situation; the agent (with some stochasticity) will produce 10 different trajectories. This *group-in-group* rollout approach ensures that the trajectories in a group are directly comparable because they start from the same state and face the same task. It’s a clever scheduling strategy: by controlling initial conditions, GiGPO can later aggregate trajectory outcomes and even match states across different rollouts. During the rollouts, everything proceeds as normal multi-step episodes (the agent interacts step by step until termination in each environment). The difference comes after collection: trajectories are partitioned into these groups for analysis. This scheduling does require the environment to be resettable to a specific state (or at least reproducibly similar conditions), which may not always be trivial, but for tasks like text-based agents or simulated environments it’s feasible.

**Handling Delayed Rewards:** GiGPO is explicitly designed to handle cases like **sparse binary rewards at the end of an episode**. If an environment only provides a reward of +1 for success (and 0 for failure) at the very end, a naive RL algorithm has a hard time telling which actions along the way contributed to success. GiGPO addresses this by computing an **episode-level relative advantage** for each trajectory in a group. Concretely, if in a group of N trajectories (all from the same start state), some succeed and some fail, each trajectory is given a score based on its total return compared to the others in the group. This tells the agent **which entire trajectory was better** in the group. For instance, if only 2 out of 10 trajectories succeeded, those get a higher episode-level advantage, encouraging the policy behaviors that led to success in those runs. This is a form of *reward normalization and scaling* that turns a sparse 0/1 reward into a graded signal (by comparing against peers). It’s similar in spirit to “relative reward” approaches or PER (Population Episodic Reward), giving the agent a sense of good vs bad outcome even if all it had was success/failure signals.

**Reward Propagation:** Beyond the episode-level signal, GiGPO introduces **step-level relative advantages** by retrospectively grouping *individual decisions* across trajectories. This is where the identical start states become useful: often, different trajectories will *visit the same state* at some point (especially early on, since they started identically). GiGPO groups actions by the state in which they were taken – effectively comparing, say, “at state S, one trajectory chose action A1 and eventually succeeded, whereas another chose A2 and failed.” By aggregating such cases, GiGPO can assign credit to specific actions: if in state S, the trajectories that picked action A1 tend to have higher final returns than those that picked A2, the algorithm gives A1 a positive **step-level advantage** over A2 for state S. This fine-grained reward propagation means even intermediate actions get feedback **before the episode ends**: the agent learns which action was better *holding the state constant*. In essence, GiGPO propagates the outcome backwards not by bootstrapping with value estimates, but by **comparing alternative action outcomes** in retrospect. The combination of episode-level and step-level advantages yields a more informative gradient: the episode-level term drives the policy to achieve overall success (long-horizon credit), while the step-level term fine-tunes the policy toward choosing the locally optimal action in each context. This addresses temporal credit assignment in a novel way – rather than relying purely on a numeric discount of a single reward, it *assigns credit by relative performance*, which is well-suited for delayed reward scenarios.

**Training Loop Modifications:** Implementing GiGPO on top of VeRL’s PPO loop required extra steps after rollout collection. Once a batch of trajectories is collected (grouped by identical initial states), the training loop computes the episode-level advantages by normalizing returns within each group. It also has to index into trajectories to find states that reoccur and group actions at those states to compute step-level advantages. Data structures like a hash map from state → list of (action, return) can be used for this. In practice, since these are LLM-based agents, the “state” could be represented by some key like a hashed conversation history or a specific turn index (the paper doesn’t detail the implementation, but conceptually that’s the idea). After computing these advantages, GiGPO combines them into an overall policy gradient update. This is a modification of the loss function: instead of the standard PPO advantage, the gradient is weighted by the **hierarchical advantages** (with some normalization to balance their contributions). The rest of the loop – optimizing model parameters – remains similar to PPO, but the objective now reflects both global and local feedback. From an infrastructure standpoint, GiGPO’s loop needs to synchronize trajectories from multiple workers if those workers produce one group each, or orchestrate that one worker runs multiple env instances for a group sequentially. It’s a bit more complex than vanilla PPO, but the VeRL framework’s flexibility (designed to allow custom algorithms) makes it possible to plug in this custom advantage calculation. In summary, VeRL-Agent’s GiGPO contributes a **robust long-horizon optimization technique**: by **grouping rollouts and propagating rewards by comparison**, it enhances multi-step learning on tasks with delayed feedback. For someone extending an RL framework, GiGPO’s techniques suggest that beyond basic n-step returns, one can incorporate *relative performance rewards* or population-based signals to significantly improve learning when reward signals are very sparse.

## Microsoft ARTIST: Multi-Step Reasoning with Tools

**ARTIST** (Agentic Reasoning and Tool Integration in Self-improving Transformers) is a framework from Microsoft Research that explicitly targets multi-step decision-making in LLM-based agents. Unlike a generic library, ARTIST is presented in a research context (with a paper and presumably internal code) focusing on how an LLM can **alternate between reasoning and acting** to solve complex tasks. The hallmark of ARTIST is enabling an LLM to **use external tools and perform multi-turn interactions** autonomously, rather than just producing a single-shot answer. This naturally requires a sophisticated multi-step rollout design.

**Temporal Abstraction and Structure:** ARTIST’s strategy for handling multi-step rollouts is to impose a clear **structure on the agent’s reasoning process**. Each rollout (episode) is not just a free-form sequence of tokens; it is segmented into distinct phases: (1) *Internal reasoning* (denoted by special `<think>...</think>` tags), (2) *Tool invocation* (e.g. `<tool_name>...` tags for calling a tool or making an API query), (3) *Tool output* (captured in `<output>...</output>` tags), and (4) *Final answer* (`<answer>...</answer>` tag). The agent (an LLM policy) decides at each step **what to do next**: continue thinking internally or invoke a tool, and which tool to use with what query. This design provides temporal abstraction in that the model isn’t forced into a fixed step duration – an internal reasoning step could be a lengthy chain-of-thought, and a tool execution might internally take multiple operations – but at the policy level these are just *one step each in the rollout*. In other words, ARTIST treats an entire segment (like “generate a thought explaining how to approach the math problem” which could be several sentences) as one action in the MDP, and similarly “execute Python code to compute something” as another action. The temporal abstraction here is **between thinking and acting**: the agent can internally reason for a while (which doesn’t advance the environment state except in its own “mind”), and then decide to act (which does advance the state via a tool). This is a form of **option-like behavior** – the internal reasoning could be seen as a “do nothing macro-action” that prepares for the next tool use. By alternating these, ARTIST effectively extends the time horizon: some steps don’t incur external changes, allowing the agent to handle very long reasoning within an episode without a flurry of external actions. Importantly, the model also learns **when to terminate** the rollout by outputting the `<answer>` tag, which is a decision that the solution is ready. This can be seen as a temporal abstraction: deciding that a multi-step sequence of actions (a plan) is complete.

**Rollout Process and Scheduling:** In ARTIST, a single rollout is an **iterative loop** where the model’s generation and tool execution alternate until the final answer is produced. Concretely, the rollout might look like: the model outputs `<think> ... </think>` (some reasoning text), then decides it needs a tool, outputs `<tool_name>query</tool_name>`, the framework calls the specified tool and gets a result, appends `<output>result</output>` to the model’s context, and the model continues. This repeats any number of times. The *rollout terminates* when the model outputs an `<answer>...</answer>` segment (or potentially if it hits some step limit). This structured interaction is fundamentally a multi-step episode. In terms of scheduling, the ARTIST paper doesn’t explicitly detail how these rollouts are executed in parallel, but given typical practice and the complexity of tasks, it’s likely that multiple rollouts are done concurrently (for example, using separate processes or threads for each, since waiting for tool API calls or code execution can be slow). The pseudo-code implied in the paper is that for each policy update iteration, they **generate several rollouts** with the current policy. Each rollout involves calls to external tools (which might be web search, code execution, etc.), so the framework must handle those external calls during generation. This essentially required building a small **environment simulator around the LLM** – the environment in ARTIST’s case is the collection of available tools and their outputs. The scheduling of tool use is *model-driven* (the model chooses when to call), but the *scheduling of rollouts* from a training perspective would involve running as many as possible in parallel to gather sufficient experience.

One challenge in ARTIST’s rollout execution is that the “environment” (tools, web, etc.) can have variable latency and is not a simple vectorized simulator. They likely adopted an asynchronous approach: issue a tool call and wait for result while perhaps other rollouts proceed. This is analogous to how SkyRL overlaps env interaction (though on a smaller scale of a single machine). The **key difference from standard RL** is that the state and action are very high-level (text and tool outputs), so the rollout scheduler must concatenate these pieces into a prompt for the model at each step. ARTIST provides a **prompt template** to ensure the format is maintained, which the rollout code uses to stitch together the conversation and feed back into the model. In summary, ARTIST’s rollout mechanism is *alternating decision and environment steps* with the environment being a suite of tools. This is a sequential dependency (can’t take a next action until the tool returns), so parallelism would come from running multiple independent rollouts at once. The framework used to implement this isn’t specified, but given Microsoft’s resources it could be a custom orchestration or something like an extension to OpenAI’s AutoGPT tooling integrated with an RL loop.

**Handling Delayed Rewards and Reward Design:** ARTIST explicitly addresses the fact that simply training an agent on final outcomes is insufficient for such complex behavior. They introduce a **composite reward function** that provides both final and intermediate feedback. The reward has three components: **Answer Reward**, **Format Reward**, and **Tool Execution Reward**. The **Answer Reward** is the primary delayed reward – a positive reward given if the final answer is correct (and presumably zero or negative if incorrect). This aligns with the ultimate task success. By itself, this is a sparse reward coming at the end of a possibly long reasoning chain. To support learning, ARTIST adds the **Format Reward**, which gives the model feedback on whether it followed the required output structure (i.e. did it use the `<think>`, `<tool>`, `<output>`, `<answer>` tags correctly and in the right order). This is a form of immediate reward available at each step (or at least, evaluable at the end by checking the log): if the model’s rollout has all the segments in the correct format, it earns this reward. Essentially, every rollout that adheres to the protocol gets a small positive reward. This incentivizes the agent to **respect the temporal structure** – an important detail because if the model deviated from the format, it could break the environment loop (e.g., failing to produce an `<answer>` tag means never ending, or producing tools in wrong order could confuse the system). The third component is the **Tool Execution Reward**, defined as the fraction of tool calls that succeeded (did not error out). For each tool invocation during the rollout, if the tool returns a valid result (e.g., code ran without exception, or the API returned data), this contributes to the reward. If the model issues malformed queries or tries to use tools incorrectly, this reward will be lower. The tool execution reward is essentially a *per-step reward* that is averaged (fraction of successes) – it encourages the agent to make valid actions.

Together, these components address delayed rewards by giving **immediate signals for desirable behavior**: maintaining structure and making valid actions. The final answer reward still handles the main objective (solving the task), but even before achieving correctness, the agent can start improving just by learning to format its output and use tools properly. This significantly eases credit assignment because instead of a single binary success/failure at the end, the agent gets a *shaped reward*. We can see this as a best practice: when facing a complex multi-step task, provide **dense feedback for sub-behaviors** that are required for success. ARTIST’s composite reward is a direct application of reward shaping aligned with the task’s process.

**Reward Propagation:** The presence of intermediate rewards (format, tool success) means that the RL algorithm can propagate credit to earlier steps naturally, since those rewards are attributed at or near the time of the behavior. For example, if a tool call fails at step 5, the negative (or zero) reward for that failure will directly affect the step-5 action’s advantage, teaching the model to choose a different action or formulate it better next time. The format reward, being given only if the entire rollout is well-structured, is computed at the end but it’s small and acts more like a bonus for following protocol; the threat of losing it likely propagates to any place the format could break (which again is under the model’s control at each step). The answer reward is only given at the very end for correct answer, which is truly delayed. ARTIST uses an RL algorithm referred to as **GRPO** (which stands for an outcome-based policy optimization method, and is likely similar to PPO) to update the policy. GRPO and PPO would handle propagation by computing the return as the sum of all these rewards (discounted appropriately). So the final answer reward will propagate back through the chain just like in VeRL/PPO, but now with additional intermediate terms. One challenge they noted is that if one naively applies the policy gradient *at every token*, the model might get unintended credit: for instance, the text of the tool’s output is something the model didn’t generate (it came from an external API), yet if you treat the whole sequence as model output, the model might try to “imitate” the tool output to gain reward. ARTIST avoids this by structuring the episode and likely only applying the RL loss on the parts the model controls (`<think>` content and the choice to call tools). The paper mentions that *uniform token-level loss on tool outputs caused the model to learn wrong behavior (imitating deterministic tool outputs) instead of learning when to invoke tools*. The fix is presumably to **exclude tool output tokens from the loss calculation**, or equivalently, only provide rewards for the decisions (not for copying the tool’s response). This nuance highlights that in multi-step scenarios, one must carefully assign credit **only to agent actions, not to environment outcomes**.

**Training Loop Integration:** Implementing ARTIST’s multi-step rollout required substantial additions to a standard LM training loop. First, the environment interface: the training framework needs to manage external tool APIs. This means during the generation of an episode, the loop must intercept special tokens (like when the model outputs `<tool_name>...`) and pause model generation to execute the tool, then resume with the tool’s output appended. In effect, the **inference process becomes interactive**. This can be done with callbacks in the text generation pipeline (e.g., detect when the model outputs a tool tag and stop to get the result). Microsoft likely built a custom loop for this, or adapted an existing agent framework with an RL algorithm. The training loop then needs to collect the entire sequence of state, action, reward tuples. Here, “state” could be represented as the dialogue/history so far, and “action” as the chunk of text the model produced at that step (tagged with think/tool/answer). Storing these for multiple steps and multiple episodes is memory-intensive but manageable with moderate horizon lengths. After rollouts are collected, the algorithm computes the total reward for each (summing answer + format + tool success rewards). Since the rewards are outcome-based, they probably use them to compute an advantage per step (perhaps using a value function; the details aren’t given but PPO usually would). Then the policy (the LLM) is updated. Notably, because the policy is an LLM, the update might use techniques from RLHF training (like using a reference model or KL penalty to stabilize, though the paper doesn’t emphasize that – it focuses on the reward design and capabilities). Another modification is that ARTIST must handle **variable-length episodes** – some tasks might require more tool uses than others. The training loop must either pad or mask trajectories for batching or handle them one by one.

In summary, ARTIST’s implementation shows that a multi-step RL loop for LLMs often looks like: *While not done: model act → environment respond*, then *compute composite reward and update policy*. The **core architectural pattern** here is an interleaved loop with a structured prompt and a custom reward function. This pattern could be highly relevant to the user’s framework: if their framework makes reward function authoring easy, adopting a similar composite reward approach would be natural. Ensuring the model adheres to a given interaction schema (like ARTIST’s prompt template) can be enforced via rewards, which is a powerful technique to keep multi-step rollouts on track. Additionally, ARTIST demonstrates the importance of having the training loop manage complex interactions (not just a simple function call to get a reward). It effectively couples an **LM acting as policy** with a **simulated environment (the tools)** within one loop – an architecture that general RL frameworks (like the user’s) can emulate by abstracting tool use as environment dynamics.

## SkyRL: Long-Horizon Multi-Turn RL Pipeline

**SkyRL** is an open-source RL training pipeline developed by Berkeley’s Sky Computing Lab, aimed at *“training real-world long-horizon agents via RL”*, particularly focusing on LLM-based agents in complex environments. SkyRL can be seen as an extension of VeRL, tailored for scenarios where an agent must carry out **multi-step plans involving tool use and environment interactions**. Examples include solving coding challenges (like the SWE-Bench benchmark for software engineering tasks) where an agent might need to read a problem, write code, run tests, debug, and iterate – a truly long-horizon, multi-tool interaction sequence. SkyRL’s design choices reflect an emphasis on *practical engineering to handle long tasks* and *flexibility in integrating various tools and environments*.

**Temporal Abstraction:** SkyRL inherits the basic temporal structure from VeRL (step-by-step actions in an episode) but introduces the concept of an **“agent layer”** that sits above the core RL loop. This agent layer is responsible for orchestrating **multiple tools and environment interactions** within an episode. While not a hierarchical policy in the classical sense, this design provides an abstraction barrier: the RL policy (the LLM) outputs some action representation (e.g., a command or choice), and the agent layer interprets it, possibly executing a sequence of low-level operations in the environment. For instance, if the LLM outputs a high-level intent like “run tests on the code,” the agent layer might actually call a testing framework, gather results, and return a summary back to the LLM. In this way, **one agent action could encapsulate a lot of environment activity**, which is a form of temporal abstraction (one decision triggers a multi-step deterministic procedure). The SkyRL blog mentions the need to handle scenarios where the model *“invoke\[s] multiple tools, write and run tests, react to feedback, and execute long-horizon plans”*. To do this, the agent likely has a set of tool APIs (similar to ARTIST’s tools) but possibly more complex (like a full coding environment). Each tool invocation might itself be considered one step, or they might allow the model to script a few steps at once (unclear, but they reference an example workflow in their Figure 2). At the very least, SkyRL’s temporal abstraction lies in its **general tool interface** – the agent isn’t restricted to one type of action (like just calling a code interpreter); it can choose among many (search the web, run code, etc.), which extends the horizon and complexity of what one “step” can achieve.

**Rollout Scheduling and Parallelism:** Because the tasks targeted by SkyRL (like SWE-Bench) can have **very long episodes** (imagine an agent coding for dozens of steps), a major focus is on efficiency. SkyRL implements **asynchronous, parallel rollouts** to utilize resources effectively. This is built on top of VeRL’s ability to do parallel rollouts, augmented with *overlap between computation and environment interaction*. For example, running code or waiting for a web response can take orders of magnitude more time than just generating text. SkyRL’s rollout scheduler likely uses a pool of worker processes/threads that handle environment interactions, and a pool for model inference, and these communicate via an event loop or task queue. As a result, while one agent is waiting for tool output, the GPU can be busy generating text for another agent’s step. This design was reported to give a **4-5× speedup** over a naive synchronous implementation on these long-horizon tasks. The scheduling is probably similar to an **actor-manager model**: the agent layer might spawn asynchronous tasks for each tool call and continue with another, but since the agent’s logic is sequential, more likely they interleave different episodes.

Imagine 100 episodes each requiring a lot of tool interaction – SkyRL would start them all and whenever an episode’s model needs to wait (say it made an API call), that worker will switch to processing another episode’s model step. In essence, this maximizes GPU utilization and hides I/O latency. This pattern is crucial for “real-world” environments where steps aren’t just fast simulator ticks. SkyRL’s reliance on VeRL (which uses Ray) suggests it might use Ray actors for environments and for model serving (possibly using SGLang or similar for fast generation). Indeed, SkyRL is explicitly a **fork of VeRL** and at the time of release was using an unreleased draft of VeRL’s SGLang async rollout feature to achieve its results. By refactoring how rollouts are handled, SkyRL ensures that even if one episode takes, say, 30 seconds of wall-clock time with many blocking calls, the trainer can still collect experiences from others in the meantime. This robust rollout scheduling is one of the **key contributions of SkyRL** as highlighted in its documentation (efficient env execution & rollout).

**Delayed Rewards and Outcome Evaluation:** In SkyRL’s target tasks, rewards are often heavily delayed. Take the coding scenario: the agent might only know if it solved the issue after writing the code and running all tests – a long sequence of actions with a payoff at the end (pass or fail). SkyRL’s framework presumably allows users to define reward functions that evaluate the final outcome (e.g., number of test cases passed, or a binary success) as well as any intermediate rewards (perhaps partial credit for passing some tests or reaching certain milestones). The blog post emphasizes *“robust long-horizon algorithms”* are needed (though not the focus of the blog), implying that the team is aware of the challenges of credit assignment with delayed rewards. In the initial SkyRL-v0 release, they likely use standard return estimation (like PPO/GAE) for propagation, similar to VeRL’s base approach. However, because they mention integrating existing algorithms and specifically cite that as an open problem, one can infer that **techniques like GiGPO or step-wise evaluation** could be on the roadmap. For now, a user of SkyRL would handle delayed rewards by writing a custom reward callback at the end of an episode (for example, compare the final state to the goal state and assign a reward). SkyRL’s code supports plugging in such custom reward functions (thanks to inheriting VeRL’s RewardManager design). There isn’t a published “composite reward” in SkyRL akin to ARTIST’s, but one could easily incorporate intermediate rewards in these tasks (e.g., reward the agent for successfully compiling code, or for each subtask done). Since the framework is built to be general, it doesn’t enforce a particular shaping; it provides the tools to implement one.

**Reward Propagation:** Given SkyRL uses algorithms from VeRL, by default it would use PPO with advantage estimation to propagate any rewards. If only a final outcome reward is given, the credit assignment problem is non-trivial. The authors mention the need for *“step-wise evaluation”* techniques (in fact, there is an unrelated work named SWEET-RL that does step-wise reward evaluation to help with long-term credit). Although not explicitly part of SkyRL-v0, the awareness of such techniques suggests they might integrate ideas like: splitting a long trajectory into segments for value estimation, using learned value functions to provide pseudo-rewards at intermediate points, or leveraging human feedback to give reward along the way. These are speculative, but generally, to propagate reward in long tasks, one can (a) increase the time horizon of credit assignment (high discount, eligibility traces), (b) add *intrinsic rewards* or sub-rewards to break up the problem, or (c) use an algorithm like GiGPO to compare trajectories. Since SkyRL is a platform, it could potentially incorporate (b) and (c) as needed. For the user’s perspective, the **generalizable method here** is to ensure your framework can accommodate either extreme: no intermediate rewards (just final) using powerful credit assignment (perhaps n-step or model-based rewards), and rich intermediate rewards if the user can define them. SkyRL’s success on tasks with only \~300 training samples hints that they likely leveraged strong reward signals (perhaps the tasks have unit tests that give fairly binary feedback – either the solution works or not, which is very sparse; yet they got improvement, possibly aided by careful reward design such as giving partial credit per test or using large models with good exploration).

**Training Loop and Architecture:** Architecturally, SkyRL is *layered on VeRL*. It “inherits \[VeRL’s] rich support for learning algorithms” – meaning any algorithm implemented in VeRL (PPO, possibly future TRPO, etc.) can be used in SkyRL. The critical addition is the **agent layer that handles the environment**. In implementation terms, this likely means SkyRL provides an environment class or wrapper that interfaces with external tasks. For example, they likely have an environment for SWE-Bench (which could involve a Docker container to run code, etc.), and the agent’s action space might be something like a textual command or code snippet. The agent layer would translate the model’s output into actions on that environment (like actually executing the code). This is analogous to how ARTIST’s loop works, but SkyRL aims to be *domain-agnostic*: they mention *“generic tool use”* and *“generic environment execution”*. In practice, they might define a standard interface (say, a JSON schema or a function call API) for tools such that new tools can be added without altering the core training loop. This modular design is important for generalizability across domains – a principle the user’s framework likely shares (since it emphasizes easy reward function authoring, it probably already decouples environment logic from reward logic).

The training loop in SkyRL, therefore, looks like: initialize a set of environment instances (could be real or simulated environments like browsers, IDEs, etc.), for each training iteration dispatch the current policy to interact with each environment until done, collect trajectories, compute rewards, then perform updates. The *asynchronous* part means this loop isn’t strictly synchronous across all envs; some might finish faster and immediately be reset to start a new episode while others are still running. This continuous pipeline resembles the *IMPALA or A3C style* of having actors generating experience continuously which a learner consumes. By overlapping, SkyRL achieves high throughput. One can imagine the implementation using something like Python `asyncio` or multi-threading where environment interactions are `await`-ed while the GPU is kept busy.

**In summary**, SkyRL demonstrates how to **extend an RL framework to real-world multi-step tasks**: by introducing an abstraction for tool use (so the policy can output generic actions), by using asynchronous parallel rollouts to handle long per-episode times, and by maintaining the flexibility of the underlying RL algorithms. It bridges the gap between *defining what the agent can do* (tool APIs, environment actions) and *learning to do it* (via RL optimization). For a framework creator, SkyRL’s approach suggests the importance of a clean separation between the RL logic and the environment logic – you can plug in complex environment interactions without changing how the algorithm works. It also shows that investing in an asynchronous architecture is worth it when adding multi-step capabilities, as naive synchronous loops would slow learning to a crawl in long-horizon scenarios.

## Comparative Analysis and Best Practices

The four projects above offer complementary insights into designing multi-step rollout mechanisms. Table 1 summarizes key features of each approach:

| **Project**                  | **Temporal Abstraction** (How actions/steps are structured)                                                                                                                                                                                                                                                   | **Rollout Scheduling** (Parallelism & orchestration)                                                                                                                                                                                                                                                                                           | **Delayed Reward Handling**                                                                                                                                                                                                                                                                                        | **Reward Propagation & Credit Assignment**                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **VeRL (Volcano Engine RL)** | Treats each turn or tool call as one step in an episode. Initially single-turn RLHF, now supports multi-turn dialogues. No explicit macro-actions beyond model-generated multi-token outputs as actions.                                                                                                      | Distributed rollout workers (Ray). Added asynchronous **multi-turn rollout** engine (SGLang) for agentic dialogues. Can run many episodes in parallel; multi-tool and multi-task training supported.                                                                                                                                           | Typically uses final outcome reward (e.g., a score from a reward model) for each trajectory. Can incorporate intermediate rewards via custom RewardManager (user-defined). Supports long horizons via high discount factors or splitting episodes.                                                                 | Standard PPO with GAE to propagate rewards back through the sequence. No specialized credit assignment by default – relies on value function to assign credit. Users can implement reward shaping to help credit (the framework is agnostic to how reward is computed).                                                                                                                                                                                       |
| **VeRL-Agent (GiGPO)**       | Same action space as VeRL (step-by-step decisions), but introduces abstraction in *evaluation*: computes trajectory-level vs step-level performance. Focuses on long-horizon credit without defining new primitives.                                                                                          | Uses **grouped rollouts**: samples multiple trajectories from the **same initial state** to enable post-hoc comparison. Still parallelizes across groups. This ensures each group experiences the same task for relative evaluation.                                                                                                           | Focus on sparse end-of-episode rewards (e.g., binary success/failure). Converts them into graded signals by comparing returns within a group (each trajectory’s return vs group mean). This provides feedback even when absolute rewards are sparse.                                                               | Two-tier reward propagation: **Episode-level advantage** rewards the agent for overall success relative to peers. **Step-level advantage** compares actions taken in the same state across different trajectories to credit or blame specific decisions. This novel credit assignment distributes delayed reward back to key actions without needing intermediate rewards.                                                                                    |
| **ARTIST (Microsoft)**       | **Structured multi-turn actions**: Alternates between “Think” (internal reasoning) and “Act” (tool use) steps. One action can be a chunk of reasoning or a single tool API call. The sequence ends with an explicit `<answer>` (termination). This enforces a high-level plan structure in the rollout.       | Interleaves model and environment in a single-threaded sequence for each rollout (due to dependency of steps). Likely runs multiple rollouts in parallel threads for throughput. Each rollout uses a fixed prompt template to maintain format. Tool calls are executed on-the-fly; the framework waits for each tool result before continuing. | **Composite reward** design: a large reward for correct final answer (delayed), plus intermediate rewards for following format and successful tool executions. This provides *dense* feedback during the rollout (for structure and tool use) to supplement the delayed answer reward.                             | Rewards are applied to the segments of the rollout under the model’s control. The training loop propagates the composite reward to the policy using an outcome-driven PPO (GRPO). By rewarding format and tool use, ARTIST ensures that some credit is assigned at each step (when format/tool are correct), not only at the end. Credit for the final answer is still propagated back using advantage estimation, but intermediate rewards make that easier. |
| **SkyRL (Berkeley)**         | Similar per-step action design as VeRL (LLM outputs an action like a tool command or text). Introduces an **agent layer** to interpret these actions across various tools/environments. This allows one decision to trigger complex sequences (e.g., running code & tests) – a form of implicit macro-action. | Emphasizes **async parallel rollouts**: many long-horizon episodes run in parallel, with overlapping computation and I/O. Achieves 4-5× speedups by not idling on env interactions. Built on VeRL’s distributed infra; can scale to real-world tasks.                                                                                          | Outcome-based rewards (e.g., task success or score) are common; the framework allows custom reward functions. Acknowledges need for robust long-horizon algorithms – users can incorporate approaches like intermediate rewards or more sophisticated returns, but initial version uses standard episodic rewards. | Inherits PPO/GAE for credit assignment. The expectation is users might plug in advanced algorithms (the mention of long-horizon algorithms not focus suggests future integration of ideas like GiGPO or others). Currently, reward propagation is via the base learner – which can be supplemented by shaping if the task provides intermediate signals (e.g., partial credit for passing some tests in SWE-Bench).                                           |

**Table 1:** Comparison of multi-step rollout strategies in VeRL, VeRL-Agent (GiGPO), ARTIST, and SkyRL. (*Citations refer to parts of the text describing each feature.*)

### Key Insights and Adaptable Techniques

Bringing multi-step RL support into a framework that already excels at reward function authoring means you have a solid foundation to build on. Here are several **best practices and architectural patterns** drawn from the above projects that you could adopt or adapt:

* **1. Structured Rollout Representation:** Define a clear format for multi-step interactions. As seen in ARTIST, having a structured prompt with designated sections for reasoning, action, and result helps manage complexity. For your framework, consider specifying an interaction schema (even if not as rigid as ARTIST’s XML tags) so that each step’s context and outcome are easy to identify. This could be as simple as a delimiter between turns, or as complex as a JSON schema for tool interactions. A structured representation will simplify **reward assignment** (you’ll know which parts of the output to evaluate for which reward) and ensure the model’s behavior stays interpretable during training.

* **2. Temporal Abstraction via Tools or Options:** While none of these frameworks implemented a full hierarchical options approach, they achieved temporal abstraction by **allowing one action to accomplish multi-step work**. In your framework, if an action can be something like “execute a script” or “perform a multi-turn dialog with a user” as a single high-level decision, this is powerful. You might implement an **option module**: for example, define a high-level action that triggers a fixed sub-policy (script) for a few steps (similar to how SkyRL’s agent layer might run a code test sequence as a response to one model command). This way, the agent can operate on multiple time scales. If your domain doesn’t naturally have such options, focus on making tool use or API calls seamless – e.g., integrate with an external tool by treating the tool’s entire operation as one environment step. This gives *temporal abstraction* (the agent decides *when* to use a tool, not how the tool does its job step-by-step). It’s essentially what ARTIST and SkyRL do with their tool integrations. Designing your framework to easily plugin new “macro” actions (tools, APIs, subroutine calls) will extend its multi-step capabilities across domains.

* **3. Asynchronous and Parallel Rollouts:** Long multi-step episodes can be slow, so follow SkyRL’s and VeRL’s lead by making your rollout generation asynchronous. Concretely, implement a mechanism where multiple environment-agent interactions can happen concurrently, and the policy learning thread can consume experiences as they come. This could involve using threading or async I/O if everything is on one machine, or a distributed task system (Ray, Celery, etc.) if across machines. The key is to **overlap waiting times with useful work**. For example, if your agent interacts with a web service and must wait for a response, ensure another agent can use the compute in the meantime. In practice, you might maintain a pool of actor processes (each holding an environment instance) that continuously generate trajectories and send them to a learner process. This architectural pattern (Actor-Learner) is common in deep RL (e.g., A3C, IMPALA) and is a good fit for multi-step scenarios. It will make your framework far more efficient for multi-step tasks without changing the core training logic.

* **4. Sophisticated Credit Assignment for Delayed Rewards:** When rewards are delayed, vanilla PPO or Q-learning might struggle. Two techniques from above can enhance this:

  * **Reward Shaping and Decomposition:** If possible, decompose the reward into parts that give **earlier feedback**. ARTIST’s composite reward is an excellent example: they added format and tool-use rewards which don’t solve the task directly but encourage behaviors that eventually lead to success. In your framework, encourage users to provide intermediate rewards for sub-goals or partial progress. Since your framework already emphasizes easy reward authoring, you can showcase recipes: e.g., “If final success is hard to achieve, add a reward for each milestone reached”. Just ensure any shaping still ultimately pushes toward the final goal (and doesn’t introduce bias; often a small weight on intermediate rewards is best).
  * **N-step Returns and Beyond:** Implement support for **n-step returns or eligibility traces** (if not already) so that users can tune how far a reward propagates back. Many RL libraries allow switching from one-step TD to n-step easily. Additionally, consider integrating the **relative advantage** idea from GiGPO. While full GiGPO is complex, a simpler variant could be: when you have a batch of trajectories, normalize their returns within the batch (so the agent focuses on outperforming the average trajectory). This could stabilize training on sparse rewards. You could also allow an option to sample the same initial state multiple times (if applicable) to facilitate comparative credit assignment. These kinds of tricks can be wrapped in your training loop as optional modules – they don’t change the environment, only how the reward signal is treated. By giving users the option to turn on “relative reward normalization” or “grouped advantage estimation”, you provide advanced credit assignment without requiring the user to implement it from scratch.

* **5. Training Loop Structure:** A multi-step episode loop will require maintaining **episode state** and possibly **history**. Borrow from how these frameworks implement their loops. For instance, you might create an `Environment` interface in your framework with methods like `reset()` and `step(action)` that returns `(next_state, reward, done, info)`. Internally, this could handle an LLM tool-using agent by encapsulating the prompt and tool outputs as the state. Designing around the standard OpenAI Gym API could help – many users are familiar with it, and it naturally handles multi-step interaction. The “state” can be complex (like a dictionary containing the conversation history, tool results, etc.), but as long as your policy can consume it and output an action, the loop can be generic. Ensure your training loop can:

  * Loop until `done` for each environment.
  * Handle vectorized environments (multiple envs at once) or async envs.
  * Accumulate rewards for the episode and apply discounting or GAE at the end.
  * Log relevant info from `info` (like how many steps, or any intermediate metrics).

  By structuring the loop cleanly, you make multi-step rollouts a first-class citizen in the framework. VeRL’s approach with separate rollout workers is one way (good for distributed settings), but a simpler single-process loop with async I/O might suffice for many cases.

* **6. Reward Function Libraries and Templates:** Since your framework already shines in reward function authoring, extend that strength to multi-step contexts by providing **templates for common reward patterns**. For example, a **delayed reward helper** that automatically gives a small penalty at each step to encourage shorter solutions (a common trick to discourage endless actions when only final reward is positive). Or a **cumulative reward wrapper** that, say, allows users to specify sub-rewards for specific events (e.g., “if tool X is used, add -0.1 reward to encourage minimal tool calls” etc.). These can be documented for users to easily plug in. The idea is to leverage your framework’s flexibility to make shaping for multi-step tasks as easy as writing a lambda function. Provide examples inspired by ARTIST (format enforcement reward) or typical robotics tasks (time penalty, fuel cost, etc.). This will help users handle delayed reward problems by *preventative design* – i.e., building a richer reward signal so that propagation is less of an issue.

* **7. Monitoring and Debugging Tools:** Multi-step rollouts are harder to debug than single-turn ones. Following these projects, it’s useful to include ways to **inspect trajectories**. For instance, ARTIST had to look at reasoning traces with tags, SkyRL likely inspects logs of tool usage. Implement logging of episodes (maybe saving a few trajectories to disk) so users can see where an agent is going wrong in a long sequence. Also, incorporate evaluation metrics that consider multi-step behavior (e.g., success rate per episode, average episode length, etc.). This isn’t directly about training, but it’s a best practice that will make developing multi-step agents easier.

* **8. Cross-Domain Generality:** The methods above are quite general (notice how similar principles are applied to LLMs and could also apply to robotics or games). Emphasize techniques that **generalize across domains**. For example, asynchronous rollouts and n-step returns are domain-agnostic. Reward shaping is domain-specific in content but general in principle. The notion of structured actions (think/act) in ARTIST is analogous to options in robotics (plan/refine). By drawing these parallels, you can ensure your framework isn’t limited to, say, NLP tasks or games alone. SkyRL explicitly combined ideas from dialogue agents and coding agents under one roof, showing generality. You might highlight that your framework can do the same: one day a user might train a dialog agent with it, the next a robot arm – as long as the environment is properly wrapped, the multi-step support and reward design utilities apply equally.

In conclusion, adding robust multi-step RL support will involve both **systems engineering** (for efficient rollouts and environment handling) and **algorithmic enhancements** (for learning from delayed rewards). VeRL and SkyRL teach us how to build scalable, flexible infrastructure (with distributed async rollouts and tool integration), while ARTIST and GiGPO (VeRL-Agent) teach us how to handle the challenges of long-term credit assignment through reward design and innovative advantage estimation. By combining these lessons – e.g., adopting an asynchronous rollout loop, offering composite or shaped rewards, and possibly implementing advanced credit assignment options – your framework can leverage its existing strength in reward authoring to excel at multi-step reinforcement learning across a variety of domains. Each of these projects ultimately aligns with the same goal: **make the agent effectively learn long sequences** without overwhelming the user or the computational resources. Adapting their best ideas will position your framework at the forefront of multi-step RL training, all while keeping it user-friendly for reward engineers.

**Sources:**

* VeRL Documentation and GitHub – *Volcano Engine Reinforcement Learning for LLMs*
* Chenyang Zhao et al., LinkedIn Announcement of Multi-turn RLHF in VeRL/SGLang
* *Group-in-Group Policy Optimization (GiGPO)* – Langfeng Zheng et al., 2025 (VeRL-Agent repository and arXiv)
* Microsoft Research, *ARTIST: Agentic Reasoning and Tool Integration in Self-improving Transformers*, 2025 (paper)
* ARTIST composite reward design – correct answer, format, tool execution rewards
* NovaSky (Berkeley Sky Lab) – *SkyRL: Train Long-Horizon Agents via RL*, 2025 (Notion blog and GitHub).
