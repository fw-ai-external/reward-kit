# Function and Tool Calling Evaluation

This guide demonstrates how to evaluate tool calls (often referred to as function calls) made by AI models using the `exact_tool_match_reward` function in Reward Kit. This reward function focuses on verifying if the AI's generated tool calls precisely match a predefined ground truth.

## Prerequisites

Before using the tool calling evaluation rewards, ensure you have:

1.  **Python 3.8+** installed on your system.
2.  **Reward Kit** installed: `pip install reward-kit`.

## Core Evaluation: `exact_tool_match_reward`

The primary method for evaluating tool calls in Reward Kit is `exact_tool_match_reward`. This function determines if the tool calls generated by an AI model in its response are an exact match to the expected tool calls defined in a ground truth.

### How It Works

1.  **Input:**
    *   `messages`: A list of message objects (either `reward_kit.models.Message` instances or dictionaries). The last message in this list is considered the AI's generation.
    *   `ground_truth`: A dictionary that must contain a `tool_calls` key. The value of this key should be a list of dictionaries, where each dictionary represents an expected tool call.

2.  **Extraction of Generated Tool Calls:**
    *   The reward function first looks for a `tool_calls` attribute on the last message object (the AI's generation). This attribute should be a list of tool call dictionaries.
    *   If `tool_calls` is not found or is empty, it attempts to parse tool calls from the `content` field of the assistant's message, looking for XML-like `<tool_call>{...}</tool_call>` structures.

3.  **Argument Deserialization:**
    *   For both generated and ground truth tool calls, the `arguments` field within each function call (which is expected to be a JSON string) is deserialized into a Python dictionary.

4.  **Comparison:**
    *   The list of generated (and deserialized) function calls is compared against the list of ground truth (and deserialized) function calls.
    *   The comparison is strict:
        *   The number of tool calls must be identical.
        *   The order of tool calls must match.
        *   For each corresponding tool call, the function `name` must be identical.
        *   The deserialized `arguments` (now dictionaries) must be identical (both keys and values).

5.  **Scoring:**
    *   If all tool calls match exactly as described above, the score is `1.0`.
    *   Otherwise, the score is `0.0`.
    *   If `ground_truth` is not provided (`None`), the score is `1.0` if the generation contains no tool calls, and `0.0` if it does.

### Example Usage

This example is based on `examples/tool_calling_example/local_eval.py` and `dataset.jsonl`.

**1. Sample `dataset.jsonl` entry:**

```json
{
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. San Francisco, CA"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location"]
        }
      }
    }
  ],
  "messages": [
    {
      "role": "user",
      "content": "What's the weather like in Boston today?"
    },
    {
      "role": "assistant",
      "content": null,
      "tool_calls": [
        {
          "id": "call_abc123",
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": "{\"location\": \"Boston, MA\", \"unit\": \"fahrenheit\"}"
          }
        }
      ]
    }
  ],
  "ground_truth": {
    "tool_calls": [
      {
        "type": "function",
        "function": {
          "name": "get_weather",
          "arguments": "{\"location\": \"Boston, MA\", \"unit\": \"fahrenheit\"}"
        }
      }
    ]
  }
}
```

**2. Python script to evaluate:**

```python
import json
from reward_kit.rewards.function_calling import exact_tool_match_reward
from reward_kit.models import Message # Optional, can also use dicts

def load_dataset(file_path: str):
    dataset = []
    with open(file_path, "r") as f:
        for line in f:
            dataset.append(json.loads(line))
    return dataset

# Assume dataset.jsonl is in the correct path
dataset = load_dataset("path/to/your/dataset.jsonl") # Adjust path as needed

for item in dataset:
    messages_data = item.get("messages", [])
    ground_truth_data = item.get("ground_truth")

    if not messages_data:
        continue

    # Optionally convert dict messages to Message objects
    # messages_objects = [Message(**msg) for msg in messages_data]

    eval_result = exact_tool_match_reward(
        messages=messages_data, # or messages_objects
        ground_truth=ground_truth_data,
    )

    print(f"Score: {eval_result.score}, Reason: {eval_result.reason}")
```

## Deprecated Functions

Previously, Reward Kit offered other function calling rewards: `schema_jaccard_reward`, `llm_judge_reward`, and `composite_function_call_reward`. These functions are now **deprecated** and internally delegate to `exact_tool_match_reward`. Their original specialized behaviors (Jaccard similarity, LLM-based judging, weighted scoring) are no longer active.

It is strongly recommended to migrate to using `exact_tool_match_reward` directly for clarity and to align with the current evaluation strategy. Using the deprecated functions will produce a `DeprecationWarning`.

## Utility: `match_function_call`

For more granular evaluation of a *single* function call against an expected JSON schema (not for comparing full tool call lists against a ground truth), the `match_function_call` utility is available.

It assesses:
*   Function name match.
*   Argument matching against a schema with varying levels of strictness (`exact`, `partial`, `flexible`/`permissive`).

This function might be useful in specific scenarios where you need to validate the schema of an individual call rather than the exactness of all calls in a sequence. Refer to its docstring for detailed parameter information.

## Best Practices for `exact_tool_match_reward`

1.  **Structure `ground_truth` Correctly:** Ensure your `ground_truth` dictionary contains the `tool_calls` key, with a list of expected tool call dictionaries.
2.  **Consistent Argument Serialization:** Arguments within tool calls (in both `messages[-1].tool_calls` and `ground_truth["tool_calls"]`) must be valid JSON strings.
3.  **Order Matters:** If multiple tool calls are expected, their order in the generated list must match the order in the `ground_truth` list.
4.  **Use `tool_calls` Attribute:** For clarity and alignment with modern standards (e.g., OpenAI API), structure the assistant's message to use the `tool_calls` attribute rather than embedding XML-like tool calls in the `content` string.

## Next Steps

-   Learn about [Creating Custom Reward Functions](../tutorials/creating_your_first_reward_function.md) if `exact_tool_match_reward` doesn't fit your specific needs.
-   Explore other [Advanced Reward Functions](advanced_reward_functions.md).
-   See general [Best Practices](../tutorials/best_practices.md) for reward function design.
