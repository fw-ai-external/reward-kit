# Integrating with Training Workflows

This guide shows how to integrate RewardKit with various training frameworks and workflows.

## Overview

RewardKit seamlessly integrates with popular training frameworks to provide reward signals for reinforcement learning and fine-tuning workflows.

## TRL Integration

RewardKit provides native support for TRL (Transformers Reinforcement Learning):

```python
from reward_kit.adapters.trl import TRLAdapter
from reward_kit import RewardFunction

# Create your reward function
@reward_function
def my_reward(response, expected_response):
    # Your reward logic here
    return score

# Convert to TRL format
trl_adapter = TRLAdapter(my_reward)
```

## Braintrust Integration

For experiment tracking and evaluation:

```python
from reward_kit.adapters.braintrust import BraintrustAdapter

# Log evaluations to Braintrust
adapter = BraintrustAdapter(project_name="my-project")
adapter.log_evaluation(reward_function, dataset)
```

## Custom Integration

You can also create custom integrations by implementing the adapter pattern:

```python
class CustomAdapter:
    def __init__(self, reward_function):
        self.reward_function = reward_function

    def adapt(self, data):
        # Your custom adaptation logic
        pass
```

## Best Practices

- Use adapters to maintain separation of concerns
- Log experiments for reproducibility
- Monitor reward distributions during training
- Validate rewards on held-out data

## Next Steps

- [TRL Integration Overview](../integrations/trl_integration_overview.mdx)
- [Braintrust Integration](../integrations/braintrust_integration.mdx)
