# External API evaluator example configuration

defaults:
  - _self_
  - override hydra/job_logging: default
  - override hydra/hydra_logging: default

hydra:
  run:
    dir: ./outputs/external_api_evaluator/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ./multirun/external_api_evaluator/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

# Dataset loading - using a more diverse dataset for testing
dataset:
  _target_: reward_kit.datasets.loader.load_and_process_dataset
  source_type: "huggingface"
  path_or_name: "imdb"  # IMDB movie reviews dataset
  config_name: "plain_text"
  split: "test"
  max_samples: 15
  column_mapping:
    user_query: text
    ground_truth_for_eval: label
  hf_extra_load_params: {}

# System prompt for generating helpful responses
system_prompt: "You are a helpful assistant. Respond to the user's input in a helpful, informative, and positive manner. Avoid any toxic or harmful content."

# Generation configuration using Fireworks
generation:
  enabled: true
  _target_: reward_kit.generation.generate_responses
  model_name: "accounts/fireworks/models/qwen3-235b-a22b"
  batch_size: 1
  max_new_tokens: 150
  temperature: 0.8
  cache:
    enabled: true
  api_params:
    rate_limit_qps: 1.0
    max_retries: 3
    max_concurrent_requests: 5

# Reward function using external API evaluator
reward:
  function_path: "main.evaluate"

# Evaluation parameters
evaluation_params:
  limit_samples: 15

# Output files
output:
  results_file: "eval_results.jsonl"
  preview_pairs_file: "preview_samples.jsonl"

logging_params:
  batch_log_interval: 2

seed: 42
verbose: true 