import json
import os
import sys
from typing import Any, Dict, List

import torch
from datasets import Dataset
from peft import LoraConfig, get_peft_model  # For PEFT
from transformers import (  # For base model
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorWithPadding,
)
from trl import (  # SFTTrainer, # Not used in this simplified example; AutoModelForCausalLMWithValueHead, # Base model loaded differently now
    GRPOConfig,
    GRPOTrainer,
)

# Ensure reward-kit is in the path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from reward_kit.models import Message
from reward_kit.rewards.math import math_reward  # Corrected: use the imported name

# --- Configuration ---
MODEL_NAME = "Qwen/Qwen2-0.5B-Instruct"  # Updated model
OUTPUT_DIR = "math_grpo_trainer_output_qwen"  # Updated output dir
DATASET_PATH = os.path.join(os.path.dirname(__file__), "dataset.jsonl")

# GRPO Configuration
grpo_config = GRPOConfig(
    # Arguments for GRPOConfig based on trl examples, ensure these are valid
    # Many common training args (batch_size, gradient_accumulation, etc.) might be expected by GRPOTrainer via this config
    # For simplicity, keeping only core GRPO/TRL args.
    # Refer to TRL's GRPOConfig documentation for all available fields.
    learning_rate=1.41e-5,  # Keep
    beta=0.1,  # Keep: Beta for GRPO
    num_train_epochs=1,  # Keep: Just a few steps for example
    max_steps=(1 if os.environ.get("TEST_MODE_TRL") == "true" else 5),  # Keep
    logging_steps=1,  # Keep
    # Removed other args that caused errors, relying on defaults or trainer handling
    # Add other necessary GRPOConfig fields if required by GRPOTrainer or your setup
    # e.g., output_dir, per_device_train_batch_size, etc. might be needed from TrainingArguments style
    # For now, this is minimal. The GRPOTrainer in trl_integration/grpo_example.py uses more.
    output_dir=OUTPUT_DIR,  # Usually needed
    remove_unused_columns=False,  # Important for keeping 'response' for reward
    per_device_train_batch_size=8,  # Adjusted to be a multiple of GRPOTrainer's default num_generations_per_prompt (8)
    gradient_accumulation_steps=1,  # Example value
    # num_generations_per_prompt is not a GRPOConfig arg, GRPOTrainer defaults to 8
    # cpu argument was invalid. Use no_cuda for TrainingArguments based configs.
    no_cuda=(True if os.environ.get("TEST_MODE_TRL") == "true" else False),
)


# --- Helper Functions ---
def load_jsonl_dataset(file_path: str):
    """Loads data from a JSONL file, expecting 'messages' field."""
    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            # We need 'query' (user prompt) and 'response' (target for SFT, or initial for GRPO)
            # For GRPO, we primarily need the query to generate responses.
            user_msg = next(
                (m["content"] for m in item["messages"] if m["role"] == "user"), None
            )
            assistant_msg = next(
                (
                    m["content"]
                    for m in item["messages"]
                    if m.get("role") == "assistant"
                ),
                None,
            )  # Optional for GRPO
            if user_msg:
                data.append(
                    {
                        "prompt": user_msg,  # Changed "query" to "prompt"
                        "response": assistant_msg if assistant_msg else "",
                    }
                )
    return data


# --- Reward Function for TRL ---


# math_reward is imported from reward_kit.rewards.math
# This adapted function will be passed to GRPOTrainer's reward_funcs
def adapted_math_reward(
    prompts: List[str], completions: List[str], **kwargs
) -> List[torch.Tensor]:  # List and torch are defined due to imports at top
    """
    Adapter function to use rk_math_reward with TRL GRPOTrainer.
    'prompts' is a list of query strings.
    'completions' are the strings generated by the model.
    'kwargs' should contain other columns from the original dataset, e.g., kwargs['response'] for ground truth.
    """
    rewards = []
    # kwargs["response"] should be a list of ground truth strings, aligned with prompts and completions
    ground_truth_responses = kwargs.get("response", [])

    if len(ground_truth_responses) != len(prompts):
        print(
            f"Warning: Length mismatch between ground_truth_responses ({len(ground_truth_responses)}) and prompts ({len(prompts)}). Rewards may be incorrect."
        )
        # Fallback or error handling if lengths don't match
        # For now, we'll proceed but this indicates a problem in data alignment from GRPOTrainer

    for i in range(len(completions)):
        user_query_str = prompts[i]
        generated_completion_str = completions[i]

        ground_truth_answer_str = ""
        if i < len(ground_truth_responses):
            ground_truth_answer_str = ground_truth_responses[i]
        else:
            print(
                f"Warning: No ground truth response found for prompt index {i}: {user_query_str}"
            )

        if not ground_truth_answer_str:
            print(
                f"Warning: Empty ground_truth_answer_str for prompt: {user_query_str}"
            )
            rewards.append(torch.tensor(0.0, dtype=torch.float32))
            continue

        messages_for_eval = [
            Message(role="user", content=user_query_str),
            Message(role="assistant", content=generated_completion_str),
        ]

        try:
            # The original_messages for math_reward is for context if needed by the reward logic,
            # but math_reward primarily uses the 'ground_truth' string for comparison.
            # For simplicity, we can pass messages_for_eval as original_messages.
            eval_result = math_reward(  # Use the imported name 'math_reward'
                messages=messages_for_eval,
                original_messages=messages_for_eval,
                ground_truth=ground_truth_answer_str,
            )
            rewards.append(torch.tensor(eval_result.score, dtype=torch.float32))
        except Exception as e:
            print(f"Error in math_reward during TRL for prompt '{user_query_str}': {e}")
            rewards.append(torch.tensor(0.0, dtype=torch.float32))  # Penalize errors
    return rewards


# --- Main Script ---
def main():
    # 1. Load Tokenizer and Model
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Load base model for PEFT
    attn_implementation_config = None
    if os.environ.get("TEST_MODE_TRL") == "true":
        print(
            "TRL Test Mode: Forcing CPU, default dtype, and eager attention for model loading."
        )
        device_map_config = "cpu"
        torch_dtype_config = None  # Let transformers/torch decide default for CPU
        attn_implementation_config = "eager"  # Force eager attention for CPU test mode
    else:
        device_map_config = "auto"
        torch_dtype_config = torch.float16
        # For non-test mode, Hugging Face Transformers will attempt to use the most optimal
        # attention mechanism available (e.g., SDPA, Flash Attention 2).
        # Explicitly: attn_implementation_config = "sdpa" or "flash_attention_2" if desired and available.

    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch_dtype_config,
        device_map=device_map_config,
        attn_implementation=attn_implementation_config,  # Pass the attention implementation config
    )

    # PEFT Configuration (LoRA)
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,  # Common practice: alpha = 2*r
        lora_dropout=0.05,
        target_modules=["q_proj", "v_proj"],  # Check these for Qwen2-0.5B
        bias="none",
        task_type="CAUSAL_LM",
    )

    policy_model = get_peft_model(base_model, lora_config)
    policy_model.print_trainable_parameters()

    # Load the dataset
    raw_dataset_data = load_jsonl_dataset(DATASET_PATH)
    if not raw_dataset_data:
        print(f"No data loaded from {DATASET_PATH}. Exiting.")
        return

    dataset = Dataset.from_list(raw_dataset_data)

    # Preprocess dataset for TRL - tokenize "prompt"
    def preprocess_function(examples):
        return tokenizer(
            examples["prompt"], truncation=True, padding="max_length", max_length=512
        )  # Changed "query" to "prompt"

    # Keep original 'prompt' and 'response' columns for the reward function
    tokenized_dataset = dataset.map(preprocess_function, batched=True)
    tokenized_dataset.set_format(
        type="torch",
        columns=[
            "input_ids",
            "attention_mask",
            "prompt",
            "response",
        ],  # Changed "query" to "prompt"
    )

    # Ensure output directory exists
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    # Logs dir is handled by TrainingArguments/GRPOConfig if report_to is set

    # GRPOTrainer instantiation
    # data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # Removed explicit data_collator
    grpo_trainer = GRPOTrainer(
        model=policy_model,  # This is now the PeftModel
        args=grpo_config,  # GRPOConfig object
        train_dataset=tokenized_dataset,
        reward_funcs=[adapted_math_reward],
        # No explicit ref_model, so policy model is its own reference
        # GRPOTrainer might handle collation internally or expect a specific dataset format.
    )

    print("Starting GRPO training loop for Math Example with PEFT...")

    # 3. Training
    # GRPOTrainer handles the training loop internally when train() is called.
    # It will use the train_dataset, reward_funcs, and args (GRPOConfig) provided.
    # Generation parameters like max_new_tokens should be part of GRPOConfig.
    # GRPOConfig has `max_completion_length`.

    # Update grpo_config with generation parameters if not already set or to override defaults
    # These were previously in a manual generation_kwargs dictionary.
    # Note: Some of these might already be default or covered by other GRPOConfig settings.
    # Check TRL's GRPOConfig documentation for the best way to set these.
    if (
        not hasattr(grpo_config, "max_completion_length")
        or grpo_config.max_completion_length is None
    ):
        grpo_config.max_completion_length = 50
    if not hasattr(grpo_config, "top_k") or grpo_config.top_k is None:
        grpo_config.top_k = 0.0
    if not hasattr(grpo_config, "top_p") or grpo_config.top_p is None:
        grpo_config.top_p = 1.0
    if not hasattr(grpo_config, "do_sample") or grpo_config.do_sample is None:
        grpo_config.do_sample = True
    # pad_token_id and eos_token_id are usually handled by the tokenizer/model config,
    # but can be set in generation_config of TrainingArguments if needed.
    # For GRPOTrainer, these might be inferred or set via specific generation args if available.

    grpo_trainer.train()

    print("\nGRPO training loop completed for Math Example.")

    # Optionally save the model
    # final_save_path = os.path.join(OUTPUT_DIR, "final_model")
    # grpo_trainer.save_model(final_save_path)
    # tokenizer.save_pretrained(final_save_path)
    # print(f"Model saved to {final_save_path}")


if __name__ == "__main__":
    # Add a check for torch and transformers availability if desired
    try:
        import datasets
        import torch
        import transformers
        import trl
    except ImportError:
        print("Error: PyTorch, Transformers, TRL, or Datasets library not found.")
        print("Please install them: pip install torch transformers trl datasets")
        sys.exit(1)

    main()
