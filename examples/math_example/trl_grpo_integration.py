import logging
import os
import sys
from typing import Any, Dict, List

import hydra  # Added
import torch
from datasets import Dataset
from omegaconf import DictConfig, OmegaConf  # Added
from peft import LoraConfig, get_peft_model  # For PEFT
from transformers import (  # For base model; DataCollatorWithPadding, # Not explicitly used with GRPOTrainer in this setup
    AutoModelForCausalLM,
    AutoTokenizer,
)
from trl import GRPOConfig, GRPOTrainer

# Ensure reward-kit is in the path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from reward_kit.common_utils import load_jsonl
from reward_kit.models import Message
from reward_kit.rewards.math import math_reward

logger = logging.getLogger(__name__)  # Added


# --- Helper Functions ---
def load_math_dataset_for_trl():
    """Loads math dataset using the new simplified dataset approach."""
    try:
        from reward_kit.datasets.loader import load_derived_dataset

        # Load the GSM8K math dataset with system prompts
        dataset = load_derived_dataset("gsm8k_math_prompts", num_samples=10)

        processed_trl_data = []
        for item in dataset:
            user_query = item.get("user_query", "")
            ground_truth = item.get("ground_truth_for_eval", "")
            system_prompt = item.get("system_prompt", "")

            if user_query:
                # Create messages format expected by TRL
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": user_query})
                if ground_truth:
                    messages.append({"role": "assistant", "content": ground_truth})

                processed_trl_data.append(
                    {
                        "prompt": user_query,
                        "response": ground_truth if ground_truth else "",
                        "messages": messages,
                    }
                )
            else:
                logger.warning(f"Skipping item due to missing user_query: {item}")

        logger.info(f"Loaded {len(processed_trl_data)} samples from GSM8K math dataset")
        return processed_trl_data

    except Exception as e:
        logger.error(f"Failed to load dataset: {e}")
        return []


# --- Reward Function for TRL ---
# math_reward is imported from reward_kit.rewards.math
# This adapted function will be passed to GRPOTrainer's reward_funcs
def adapted_math_reward(
    prompts: List[str], completions: List[str], **kwargs
) -> List[torch.Tensor]:  # List and torch are defined due to imports at top
    """
    Adapter function to use rk_math_reward with TRL GRPOTrainer.
    'prompts' is a list of query strings.
    'completions' are the strings generated by the model.
    'kwargs' should contain other columns from the original dataset, e.g., kwargs['response'] for ground truth.
    """
    rewards = []
    ground_truth_responses = kwargs.get("response", [])

    if len(ground_truth_responses) != len(prompts):
        logger.warning(
            f"Length mismatch between ground_truth_responses ({len(ground_truth_responses)}) and prompts ({len(prompts)}). Rewards may be incorrect."
        )

    for i in range(len(completions)):
        user_query_str = prompts[i]
        generated_completion_str = completions[i]
        ground_truth_answer_str = ""

        if i < len(ground_truth_responses):
            ground_truth_answer_str = ground_truth_responses[i]
        else:
            logger.warning(
                f"No ground truth response found for prompt index {i}: {user_query_str}"
            )

        if not ground_truth_answer_str:  # Check if ground_truth is empty or None
            logger.warning(
                f"Empty ground_truth_answer_str for prompt: {user_query_str}. Assigning 0 reward."
            )
            rewards.append(torch.tensor(0.0, dtype=torch.float32))
            continue

        messages_for_eval = [
            Message(role="user", content=user_query_str),
            Message(role="assistant", content=generated_completion_str),
        ]

        try:
            eval_result = math_reward(
                messages=messages_for_eval,
                ground_truth=ground_truth_answer_str,
            )
            rewards.append(torch.tensor(eval_result.score, dtype=torch.float32))
        except Exception as e:
            logger.error(
                f"Error in math_reward for prompt '{user_query_str}': {e}",
                exc_info=True,
            )
            rewards.append(torch.tensor(0.0, dtype=torch.float32))
    return rewards


# --- Main Script ---
@hydra.main(config_path="conf", config_name="trl_grpo_config", version_base=None)
def main(cfg: DictConfig) -> None:
    logger.info(f"Hydra configuration:\n{OmegaConf.to_yaml(cfg)}")
    logger.info(f"Current working directory: {os.getcwd()}")
    logger.info(
        f"Hydra output directory: {hydra.core.hydra_config.HydraConfig.get().runtime.output_dir}"
    )

    # --- Configuration from Hydra ---
    MODEL_NAME = cfg.model_name

    max_steps_config = 1 if cfg.test_mode_trl else cfg.grpo.max_steps

    grpo_config = GRPOConfig(
        output_dir=hydra.core.hydra_config.HydraConfig.get().runtime.output_dir,
        learning_rate=cfg.grpo.learning_rate,
        beta=cfg.grpo.beta,
        num_train_epochs=cfg.grpo.num_train_epochs,
        max_steps=max_steps_config,
        logging_steps=cfg.grpo.logging_steps,
        remove_unused_columns=False,  # Crucial for keeping 'response' for reward_funcs
        per_device_train_batch_size=cfg.grpo.per_device_train_batch_size,
        gradient_accumulation_steps=cfg.grpo.gradient_accumulation_steps,
        no_cuda=cfg.test_mode_trl,
        max_completion_length=cfg.grpo.max_completion_length,
        top_k=cfg.grpo.top_k,
        top_p=cfg.grpo.top_p,
        # Note: do_sample is a generation parameter, not a GRPOConfig parameter
        # Other TRL/TrainingArguments can be added here from cfg if needed
    )
    logger.info(f"GRPOConfig prepared: {grpo_config}")
    # 1. Load Tokenizer and Model
    logger.info(f"Loading tokenizer for model: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        logger.info(
            f"Tokenizer pad_token was None, set to eos_token: {tokenizer.eos_token}"
        )

    # Load base model for PEFT
    logger.info(f"Loading base model: {MODEL_NAME}")
    attn_implementation_config = None
    if cfg.test_mode_trl:  # Use cfg.test_mode_trl
        logger.info(
            "TRL Test Mode: Forcing CPU, default dtype, and eager attention for model loading."
        )
        device_map_config = "cpu"
        torch_dtype_config = None
        attn_implementation_config = "eager"
    else:
        device_map_config = "auto"
        torch_dtype_config = torch.float16  # Consider making this configurable
        # attn_implementation_config can be "sdpa" or "flash_attention_2" if available
        # For now, let transformers decide by passing None if not "eager"
        logger.info(
            f"Using device_map: {device_map_config}, torch_dtype: {torch_dtype_config}, attn_implementation: {attn_implementation_config}"
        )

    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch_dtype_config,
        device_map=device_map_config,
        attn_implementation=attn_implementation_config,
    )
    logger.info(f"Base model {MODEL_NAME} loaded.")

    # PEFT Configuration (LoRA) - consider moving to Hydra config, e.g. cfg.lora.r etc.
    lora_config = LoraConfig(
        r=8,  # Example, make configurable: cfg.lora.r
        lora_alpha=16,  # Example, make configurable: cfg.lora.alpha
        lora_dropout=0.05,  # Example, make configurable: cfg.lora.dropout
        target_modules=[
            "q_proj",
            "v_proj",
        ],  # Example, make configurable: cfg.lora.target_modules
        bias="none",  # Example, make configurable: cfg.lora.bias
        task_type="CAUSAL_LM",  # Example, make configurable: cfg.lora.task_type
    )
    logger.info(f"LoRA config: {lora_config}")

    policy_model = get_peft_model(base_model, lora_config)
    trainable_params, all_param = policy_model.get_nb_trainable_parameters()
    logger.info(
        f"Trainable PEFT params: {trainable_params}. All params: {all_param}. Percentage: {100 * trainable_params / all_param:.2f}%"
    )

    # Load the dataset using new simplified approach
    logger.info("Loading math dataset using simplified dataset architecture")
    raw_dataset_data = load_math_dataset_for_trl()
    if not raw_dataset_data:
        logger.error("No data loaded from math dataset. Exiting.")
        return

    dataset = Dataset.from_list(raw_dataset_data)
    logger.info(f"Dataset loaded with {len(dataset)} samples.")

    # Preprocess dataset for TRL - tokenize "prompt"
    def preprocess_function(examples):
        # Consider making max_length configurable, e.g., cfg.tokenizer.max_length
        return tokenizer(
            examples["prompt"], truncation=True, padding="max_length", max_length=512
        )

    logger.info("Tokenizing dataset...")
    tokenized_dataset = dataset.map(preprocess_function, batched=True)
    tokenized_dataset.set_format(
        type="torch",
        columns=[
            "input_ids",
            "attention_mask",
            "prompt",
            "response",
        ],
    )
    logger.info("Dataset tokenized and formatted.")

    # GRPOTrainer instantiation
    grpo_trainer = GRPOTrainer(
        model=policy_model,
        args=grpo_config,  # This is the GRPOConfig object created from cfg
        train_dataset=tokenized_dataset,
        reward_funcs=[adapted_math_reward],
        # tokenizer=tokenizer, # Pass if needed, TRL might infer from model
    )
    logger.info("GRPOTrainer instantiated.")

    logger.info("Starting GRPO training loop...")
    grpo_trainer.train()
    logger.info("GRPO training loop completed for Math Example.")

    # Model is automatically saved by GRPOTrainer to its args.output_dir (Hydra's output dir)
    # Optionally, save tokenizer explicitly if not handled by trainer or if specific path needed.
    tokenizer_save_path = hydra.core.hydra_config.HydraConfig.get().runtime.output_dir
    tokenizer.save_pretrained(tokenizer_save_path)
    logger.info(f"Tokenizer saved to {tokenizer_save_path}")


if __name__ == "__main__":
    # This block is for attempting to run the script directly, not via Hydra CLI.
    # Hydra's @hydra.main decorator changes how main() is called.
    # Running `python script_name.py` will typically show Hydra's help message.
    # To properly run with Hydra, use `python script_name.py --config-name=trl_grpo_config`

    # Basic import check for user convenience if trying to run directly.
    try:
        import accelerate
        import datasets
        import peft
        import torch
        import transformers
        import trl
    except ImportError as e:
        # Using print for immediate visibility if logger isn't set up without Hydra
        print(f"Import error: {e}. Some required libraries are missing.")
        print(
            "Please ensure PyTorch, Transformers, TRL, Datasets, PEFT, and Accelerate are installed."
        )
        print("Example: pip install 'reward-kit[trl]' transformers")
        sys.exit(1)

    # Calling main() directly here will invoke Hydra's initialization process.
    # If no command-line arguments are provided to specify the config, Hydra might error
    # or use defaults if `config_path` in @hydra.main is structured to allow it.
    main()
