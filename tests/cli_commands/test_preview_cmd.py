import pytest
import json
import requests 
from unittest.mock import patch, MagicMock
from pathlib import Path 

from reward_kit.cli_commands import preview as preview_cmd_module 
from reward_kit.cli_commands.preview import preview_command
from reward_kit.models import EvaluateResult, MetricResult, Message
from reward_kit.generic_server import EvaluationRequest 

try:
    from datasets import Dataset, IterableDataset, DatasetDict, IterableDatasetDict
    DATASETS_AVAILABLE = True
except ImportError: 
    DATASETS_AVAILABLE = False
    class _DummyDSType: pass
    IterableDataset = _DummyDSType 

class MockArgs:
    def __init__(self, **kwargs):
        self.verbose = False
        self.metrics_folders = None
        self.samples = None
        self.max_samples = 5
        self.huggingface_dataset = None
        self.huggingface_split = "train"
        self.huggingface_prompt_key = "prompt"
        self.huggingface_response_key = "response"
        self.huggingface_key_map = None
        self.remote_url = None
        self.__dict__.update(kwargs)

@pytest.fixture
def mock_check_environment():
    with patch('reward_kit.cli_commands.preview.check_environment', return_value=True) as mock_check:
        yield mock_check

def create_temp_jsonl(tmp_path: Path, samples_data: list) -> str:
    sample_file_path = tmp_path / "temp_samples.jsonl"
    with open(sample_file_path, "w", encoding="utf-8") as f:
        for sample in samples_data:
            f.write(json.dumps(sample) + "\n")
    return str(sample_file_path)

class TestPreviewCommandRemoteUrl:

    @patch('requests.post')
    def test_preview_remote_url_success_with_file(self, mock_post, mock_check_environment, tmp_path, capsys):
        mock_response = MagicMock()
        mock_response.status_code = 200
        sample_data_for_file = [
            {"messages": [{"role": "user", "content": "User prompt 1"}, {"role": "assistant", "content": "Assistant response 1"}],
             "ground_truth": "GT 1", "custom_kwarg": "custom_val_1"}
        ]
        temp_sample_file = create_temp_jsonl(tmp_path, sample_data_for_file)

        eval_result_payload = EvaluateResult(
            score=0.8, reason="Remote success", is_score_valid=True, 
            metrics={"accuracy": MetricResult(score=0.9, reason="High acc", is_score_valid=True)} # This already has metrics
        ).model_dump()
        mock_response.json.return_value = eval_result_payload
        mock_post.return_value = mock_response

        args = MockArgs(remote_url="http://fake-remote-eval.com", samples=temp_sample_file, max_samples=1)
        return_code = preview_command(args)
        assert return_code == 0
        
        expected_endpoint = "http://fake-remote-eval.com/evaluate"
        expected_payload_sample1 = EvaluationRequest(
            messages=sample_data_for_file[0]["messages"],
            ground_truth=sample_data_for_file[0]["ground_truth"], 
            kwargs={"custom_kwarg": sample_data_for_file[0]["custom_kwarg"]}
        ).model_dump()
        mock_post.assert_called_once_with(expected_endpoint, json=expected_payload_sample1, timeout=30)
        
        captured = capsys.readouterr()
        assert "Previewing against remote URL: http://fake-remote-eval.com" in captured.out
        assert "--- Sample 1 ---" in captured.out
        assert "Score: 0.8" in captured.out

    @pytest.mark.skipif(not DATASETS_AVAILABLE, reason="datasets library not installed")
    @patch('datasets.load_dataset') 
    @patch('requests.post')
    def test_preview_remote_url_success_with_hf(self, mock_post, mock_hf_load_dataset, mock_check_environment, capsys):
        hf_sample_data = [
            {"prompt": "HF User prompt", "response": "HF Assistant response", "ground_truth_col": "HF GT"}
        ]
        mock_iterable_ds = MagicMock(spec=IterableDataset)
        mock_iterable_ds.__iter__.return_value = iter(hf_sample_data)
        mock_hf_load_dataset.return_value = mock_iterable_ds

        mock_response = MagicMock()
        mock_response.status_code = 200
        # Corrected: Explicitly provide metrics={}
        eval_result_payload = EvaluateResult(score=0.7, reason="HF Remote success", metrics={}).model_dump()
        mock_response.json.return_value = eval_result_payload
        mock_post.return_value = mock_response

        args = MockArgs(
            remote_url="http://fake-hf-eval.com", 
            huggingface_dataset="test/hf-dataset",
            huggingface_prompt_key="prompt",
            huggingface_response_key="response",
            huggingface_key_map=json.dumps({"ground_truth_col": "ground_truth"}),
            max_samples=1
        )
        return_code = preview_command(args)
        assert return_code == 0

        expected_payload = EvaluationRequest(
            messages=[{"role": "user", "content": "HF User prompt"}, {"role": "assistant", "content": "HF Assistant response"}],
            ground_truth="HF GT",
            kwargs={} 
        ).model_dump()
        mock_post.assert_called_once_with("http://fake-hf-eval.com/evaluate", json=expected_payload, timeout=30)
        captured = capsys.readouterr()
        assert "Score: 0.7" in captured.out


    @patch('requests.post')
    def test_preview_remote_url_http_error(self, mock_post, mock_check_environment, tmp_path, capsys):
        sample_data = [{"messages": [{"role": "user", "content": "Test"}]}]
        temp_sample_file = create_temp_jsonl(tmp_path, sample_data)
        mock_post.side_effect = requests.exceptions.HTTPError("403 Client Error: Forbidden for url")
        
        args = MockArgs(remote_url="http://fake-remote-eval.com", samples=temp_sample_file, max_samples=1)
        return_code = preview_command(args)
        assert return_code == 0 
        
        captured = capsys.readouterr()
        assert "Error calling remote URL" in captured.out
        assert "403 Client Error: Forbidden for url" in captured.out

    def test_preview_remote_url_invalid_url_format(self, mock_check_environment, tmp_path, capsys):
        sample_data = [{"messages": [{"role": "user", "content": "Test"}]}]
        temp_sample_file = create_temp_jsonl(tmp_path, sample_data)
        args = MockArgs(remote_url="ftp://invalid-url.com", samples=temp_sample_file)
        return_code = preview_command(args)
        assert return_code == 1
        captured = capsys.readouterr()
        assert "Error: Invalid --remote-url 'ftp://invalid-url.com'" in captured.out

    def test_preview_remote_url_no_samples_provided(self, mock_check_environment, capsys):
        args = MockArgs(remote_url="http://fake-remote-eval.com") 
        return_code = preview_command(args)
        assert return_code == 1
        captured = capsys.readouterr()
        assert "Error: Either sample file (--samples) or HuggingFace dataset (--huggingface-dataset) is required." in captured.out
    
    def test_preview_remote_url_sample_file_not_found(self, mock_check_environment, capsys):
        args = MockArgs(remote_url="http://fake-remote-eval.com", samples="non_existent.jsonl")
        return_code = preview_command(args)
        assert return_code == 1
        captured = capsys.readouterr()
        assert "Error: Sample file 'non_existent.jsonl' not found" in captured.out


class TestPreviewCommandLocalMode:
    @patch('reward_kit.cli_commands.preview.preview_evaluation') 
    def test_preview_local_mode_success(self, mock_preview_eval, mock_check_environment, tmp_path, capsys):
        sample_data = [{"messages": [{"role": "user", "content": "Test"}]}]
        temp_sample_file = create_temp_jsonl(tmp_path, sample_data)
        
        mock_preview_result = MagicMock()
        mock_preview_eval.return_value = mock_preview_result
        
        args = MockArgs(metrics_folders=["mf=path"], samples=temp_sample_file)
        return_code = preview_command(args)
        
        assert return_code == 0
        mock_preview_eval.assert_called_once()
        mock_preview_result.display.assert_called_once()

    def test_preview_local_mode_missing_metrics_folders(self, mock_check_environment, tmp_path, capsys):
        sample_data = [{"messages": [{"role": "user", "content": "Test"}]}]
        temp_sample_file = create_temp_jsonl(tmp_path, sample_data)
        args = MockArgs(samples=temp_sample_file) 
        
        return_code = preview_command(args)
        assert return_code == 1
        captured = capsys.readouterr()
        assert "Error: Either --remote-url or --metrics-folders must be specified." in captured.out
